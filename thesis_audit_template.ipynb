{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Thesis Audit Template \u2014 Cyberhate Detection\n",
        "Pipeline: **Train \u2192 Calibrate \u2192 Fairness \u2192 Functional \u2192 Robustness \u2192 Faithfulness \u2192 Risk\u2013Coverage**."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import os, subprocess, numpy as np, pandas as pd\n",
        "run_dir = 'runs/roberta_bin'  # set to your trained run\n",
        "print('Using run_dir=', run_dir)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Calibration"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "cmd = ['python','scripts/calibrate.py','--logits',f'{run_dir}/val_logits.npy','--labels',f'{run_dir}/val_labels.npy','--out',f'{run_dir}/val_probs_cal.npy']\n",
        "print(' '.join(cmd)); subprocess.run(cmd, check=True)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Fairness (BPSN/BNSP/Subgroup AUC)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "preds_csv = 'eval/preds_with_identities.csv'  # prepare externally\n",
        "cmd = ['python','scripts/eval.py','--preds',preds_csv,'--id-cols','identity_women,identity_muslim,identity_black']\n",
        "print(' '.join(cmd)); subprocess.run(cmd, check=True)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Functional Tests (HateCheck)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "cmd = ['python','scripts/run_hatecheck.py','--model-path',run_dir,'--hatecheck','data/hatecheck.csv']\n",
        "print(' '.join(cmd)); subprocess.run(cmd, check=True)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Robustness: OCR/ASR Noise Sweep (prepare noisy eval CSV)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from cyberhate_lab.ocr_asr.noise_simulation import batch_corrupt\n",
        "df = pd.read_csv('eval/eval_texts.csv')  # id,text,label\n",
        "df['text_noisy'] = batch_corrupt(df['text'].astype(str).tolist(), p_del=0.03, p_sub=0.03, p_ins=0.03)\n",
        "df.to_csv('eval/eval_texts_noisy.csv', index=False)\n",
        "print('Wrote eval/eval_texts_noisy.csv')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Faithfulness: Train spans then evaluate"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Train spans externally: python -m cyberhate_lab.xai.train_spans --train data/spans_train.csv --val data/spans_val.csv --model roberta-base --out runs/span_model\n",
        "cmd = ['python','scripts/eval_rationales.py','--model-path',run_dir,'--val-csv','data/spans_val.csv','--rationales-npy','runs/span_model/val_token_rationales.npy']\n",
        "print(' '.join(cmd))\n",
        "try:\n",
        "    subprocess.run(cmd, check=True)\n",
        "except Exception as e:\n",
        "    print('Run span training first; then re-run this cell.')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Risk\u2013Coverage"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "cmd = ['python','scripts/risk_coverage.py','--probs',f'{run_dir}/val_probs.npy','--labels',f'{run_dir}/val_labels.npy','--out',f'{run_dir}/risk_coverage.png']\n",
        "print(' '.join(cmd)); subprocess.run(cmd, check=True)\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}